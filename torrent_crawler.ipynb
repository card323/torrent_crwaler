{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/card323/torrent_crwaler/blob/main/torrent_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Python Web Crawler\n",
        "\n",
        "**Understanding the Task:**\n",
        "We'll create a Python script that:\n",
        "1. Takes a website URL as input.\n",
        "2. Crawls the website, following links.\n",
        "3. Extracts relevant data from the pages.\n",
        "4. Outputs the data in a structured format (e.g., JSON, CSV).\n",
        "\n",
        "**Required Libraries:**\n",
        "* **requests:** For making HTTP requests to fetch web pages.\n",
        "* **Beautiful Soup:** For parsing HTML content.\n",
        "* **urllib.parse:** For URL manipulation.\n",
        "\n",
        "**Code:**"
      ],
      "metadata": {
        "id": "M72nXijJeaH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, quote\n",
        "import urllib.parse\n",
        "from datetime import date\n",
        "\n",
        "def crawl_website(url,blocklist=[], max_depth=1):\n",
        "    \"\"\"Crawls a website and extracts data.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the website to crawl.\n",
        "        blocklist (list): A list of URLs to avoid crawling.\n",
        "        max_depth (int): The maximum depth to crawl.\n",
        "    \"\"\"\n",
        "\n",
        "    visited_urls = set()\n",
        "    data = []\n",
        "\n",
        "    def crawl_page(url, depth):\n",
        "        if url in visited_urls or any(re.match(pattern, url) for pattern in blocklist)  or depth > max_depth:\n",
        "            return\n",
        "        visited_urls.add(url)\n",
        "\n",
        "        try:\n",
        "            print(f\"crawling {url}\")\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()  # Raise an exception for error status codes\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            elements = soup.find_all('div', class_='card mb-3')\n",
        "            # Extract data from the page (adjust selectors based on website structure)\n",
        "            for element in elements:\n",
        "            # Extract data from each element\n",
        "              title_element = element.find('h5', class_='title is-4 is-spaced')\n",
        "              title_link = title_element.find('a')\n",
        "              title = title_link.text.strip() if title_link else None\n",
        "\n",
        "              pic_element = element.find('img', class_='image')\n",
        "              pic_url = pic_element['src'] if pic_element else None\n",
        "\n",
        "              # Extract torrent link from magnet link button\n",
        "              magnet_link_element = element.find('a', class_='button is-primary is-fullwidth', title='Magnet torrent')\n",
        "              magnet_link = magnet_link_element['href'] if magnet_link_element else None\n",
        "\n",
        "              # Extract torrent download link\n",
        "              download_link_element = element.find('a', class_='button is-primary is-fullwidth', title='Download .torrent')\n",
        "              download_link = download_link_element['href'] if download_link_element else None\n",
        "              if download_link and not download_link.startswith('http'):\n",
        "                  download_link = urllib.parse.urljoin(url, download_link)\n",
        "              # Add data to list\n",
        "              data.append({'title': title, 'pic': pic_url, 'magnet_link': magnet_link, 'download_link': download_link})\n",
        "\n",
        "\n",
        "            # Find and follow links\n",
        "            for link in soup.find_all('a'):\n",
        "                href = link.get('href')\n",
        "                if href and href.startswith('http'):\n",
        "                    absolute_url = urljoin(url, href)\n",
        "                    crawl_page(absolute_url,depth+1)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error crawling {url}: {e}\")\n",
        "\n",
        "    crawl_page(url,max_depth)\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "with open('actress_names.txt', 'r') as f:\n",
        "    actress_names = [line.strip() for line in f]\n",
        "website_urls = [f\"https://www.141jav.com/actress/{quote(name)}\" for name in actress_names]\n",
        "new_data = []\n",
        "blocklist = [r\"https://.*\\.shopify\\.com/.*\",r\"https://.*\\.skilljar\\.com/.*\"]\n",
        "for url in website_urls:\n",
        "    new_data.extend(crawl_website(url, blocklist, 1))\n",
        "\n",
        "# Load archived data\n",
        "try:\n",
        "    with open('archived_data.json', 'r') as f:\n",
        "        archived_data = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    archived_data = []\n",
        "\n",
        "# Find elements in new_data but not in archived_data\n",
        "def is_not_in_archived(item, archived_data):\n",
        "    return not any(item['title'] == a['title'] for a in archived_data)\n",
        "\n",
        "new_elements = [item for item in new_data if is_not_in_archived(item, archived_data)]\n",
        "\n",
        "# Update archived data\n",
        "archived_data.extend(new_elements)\n",
        "\n",
        "# Save updated archived data\n",
        "with open('archived_data.json', 'w') as f:\n",
        "    json.dump(archived_data, f, indent=4)\n",
        "\n",
        "# Output the extracted data (e.g., as JSON)\n",
        "import json\n",
        "today = date.today().strftime(\"%Y-%m-%d\")\n",
        "filename = f\"extracted_data_{today}.json\"\n",
        "# Output the extracted data\n",
        "with open(filename, 'w') as f:\n",
        "    json.dump(new_elements, f, indent=4)\n",
        "\n",
        "def generate_html(data):\n",
        "    html = \"<!DOCTYPE html><html><head><title>Crawled Data</title></head><body>\"\n",
        "    for item in data:\n",
        "        html += f\"<h2>{item['title']}</h2>\"\n",
        "        if item['pic']:\n",
        "            html += f\"<img src='{item['pic']}'><br>\"\n",
        "        if item['download_link']:\n",
        "            html += f\"<a href='{item['download_link']}'>Download</a><br>\"\n",
        "        if item['magnet_link']:\n",
        "            html += f\"<a href='{item['magnet_link']}'>Magnet</a><br>\"\n",
        "        html += \"<hr>\"\n",
        "    html += \"</body></html>\"\n",
        "    return html\n",
        "\n",
        "with open(filename, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "html_content = generate_html(data)\n",
        "\n",
        "with open('output.html', 'w') as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(\"HTML file generated: output.html\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crawling https://www.141jav.com/actress/Nagi%20Hikaru\n",
            "crawling https://www.141jav.com/actress/Mita%20Marin\n",
            "crawling https://www.141jav.com/actress/Yoshitaka%20Nene\n",
            "crawling https://www.141jav.com/actress/Arata%20Arina\n",
            "crawling https://www.141jav.com/actress/Yatsugake%20Umi\n",
            "crawling https://www.141jav.com/actress/Miyashita%20Rena\n",
            "crawling https://www.141jav.com/actress/Sakura%20Momo\n",
            "crawling https://www.141jav.com/actress/Shidara%20Yuuhi\n",
            "crawling https://www.141jav.com/actress/Kaede%20Karen\n",
            "crawling https://www.141jav.com/actress/Neo%20Akari\n",
            "crawling https://www.141jav.com/actress/Momonogi%20Kana\n",
            "HTML file generated: output.html\n"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNOiJQB4eaH6",
        "outputId": "c16127df-dbda-490e-d5ff-db23aa29119b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**:\n",
        "\n",
        "1. **Import necessary libraries**: We import requests, BeautifulSoup, urljoin, quote, json, and date.\n",
        "2. **Define crawl_website function**: This function takes a URL, a blocklist of URLs to avoid, and a maximum depth as input, and crawls the website to extract data.\n",
        "3. **Define generate_html function**: This function\n",
        "takes the extracted data and generates an HTML page with the data.\n",
        "4. **Read actress names from file**: We read actress names from a file named 'actress_names.txt'.\n",
        "5. **Generate website URLs**: We generate website URLs from the actress names, encoding them for use in URLs.\n",
        "6. **Load archived data**: We load previously archived data from 'archived_data.json', or initialize an empty list if the file doesn't exist.\n",
        "7. **Extract new data**: We crawl the websites and extract new data.\n",
        "8. **Find new elements**: We compare the new data with the archived data and find elements that are not present in the archived data.\n",
        "9. **Update archived data**: We append the new elements to the archived data.\n",
        "10. **Save updated archived data**: We save the updated archived data to 'archived_data.json'.\n",
        "11. **Generate filename with today's date**: We generate a filename for the extracted data with today's date in the format \"extracted_data_YYYY-MM-DD.json\".\n",
        "12. **Save extracted data**: We save the extracted data to the generated filename.\n",
        "13. **Generate HTML**: We generate an HTML page from the extracted data and save it to 'output.html'.\n",
        "\n",
        "\n",
        "Customization:\n",
        "\n",
        "* Data extraction: Adjust the selectors (soup.find(...)) in the crawl_website function based on the specific structure of the websites you're crawling.\n",
        "* Blocklist: Update the blocklist to include any URLs you want to avoid crawling.\n",
        "Maximum depth: Change the max_depth parameter to control how deep you want to crawl the websites.\n",
        "* HTML generation: Modify the generate_html function to customize the appearance of the generated HTML page.\n",
        "* Data filtering: You can add additional logic to filter the extracted data based on specific criteria.\n",
        "* Error handling: Implement more robust error handling to handle potential issues during the crawling process.\n",
        "* Rate limiting: Consider adding rate limiting to avoid overwhelming the target websites."
      ],
      "metadata": {
        "id": "0xkkturieaH7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}